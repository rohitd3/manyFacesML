{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is test accuracy?\n",
    "In PyTorch, the term \"test accuracy\" typically refers to the accuracy of a trained model on a separate test dataset. It's used to evaluate the performance of a machine learning model. Test accuracy measures how well the model generalizes to unseen data (data that the model hasn't interacted with or seen yet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is test accuracy?\n",
    "In PyTorch, the term \"test accuracy\" typically refers to the accuracy of a trained model on a separate test dataset. It's used to evaluate the performance of a machine learning model. Test accuracy measures how well the model generalizes to unseen data (data that the model hasn't interacted with or seen yet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of test accuracy\n",
    "Through training, the model is able to learn in order to better its own accuracy. This is paramount to machine learning; in context of our project, the model's accuracy in being able to identify what certain images represent is what our project is based on"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "In this part of our project, we are importing the optim module from the pytorch library, which allows us to use optimization algorithms. We then define the loss function to be criterion, with the use of CrossEntropyLoss(), which is used for multi-class classification problems. Then we initialize the optimizer, with a learning rate of .001, and a momentum of .9. The learning rate determines the step size at each iteration, and momentum helps accelerate convergence by considering previous gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder\n",
    "Samuel this is you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running loss incremented by loss of mini-batches\n",
    "\n",
    "The item() method is used to take the loss value of the mini-batch that is currently being analyzed and add it to the value of the running loss. Then it checks if the current iteration is a multiple of 2000, which is used to print the average running loss every few iterations, allowing for the training progress to be monitored. It then prints the current epoch number, the iteration number, and the average running loss over the past 2000 iterations. Lastly, after the average running loss is printed, it is reset to 0 to calculate the loss for the next iterations. With the model training for 5 epochs, the training is only finished after it trains for 5 epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this is to see the training progress because the average running loss is printed periodically, meaning that if the loss is decreasing over the epochs. The model is learning and improving itself, aka MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #  Start of running loss\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
