{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Identifying faces\n",
        "\n",
        "> we identify faces\n",
        "\n",
        "- toc: false \n",
        "- comments: true\n",
        "- categories: [python]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTWzqruQpyE7",
        "outputId": "d1d7e64a-9a68-4e7a-bed9-3d9ac556e7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torchviz \n",
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "# the Pandas mismatch hasn't been a problem, because we aren't using it in our code\n",
        "#   ERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 1.1.0 which is incompatible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPjCs6GFE6dg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "restart_runtime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "GxMYvgzpCqrE",
        "outputId": "92d916a0-4c59-42fc-d0c6-c2a96e86ce13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            " ImageWidth =  384\n",
            " ImageHeight =  384\n",
            " width1 =  190\n",
            " height1 =  190\n",
            " width2 =  93\n",
            " height2 =  93\n",
            "\n",
            "\n",
            " ****************  START LOADING DATA AND PROCESSING ******************* \n",
            "num_data =  400\n",
            "\n",
            " train_idx =  [233, 261, 54, 14, 246, 47, 309, 360, 215, 189, 131, 22, 252, 25, 186, 164, 66, 253, 351, 84, 33, 280, 155, 334, 133, 137, 239, 90, 3, 288, 219, 106, 160, 234, 313, 210, 172, 220, 74, 180, 173, 248, 391, 389, 8, 35, 70, 62, 130, 369, 39, 107, 328, 376, 48, 96, 310, 312, 316, 78, 322, 135, 16, 81, 147, 355, 341, 191, 365, 42, 257, 188, 176, 283, 317, 128, 352, 153, 41, 353, 386, 15, 304, 17, 20, 179, 2, 238, 320, 91, 292, 77, 113, 354, 331, 291, 300, 27, 101, 152, 4, 229, 80, 242, 230, 86, 209, 255, 357, 208, 34, 75, 104, 5, 375, 216, 293, 221, 166, 398, 139, 149, 314, 59, 199, 350, 387, 228, 323, 125, 298, 315, 264, 67, 60, 397, 26, 218, 142, 19, 124, 192, 185, 243, 311, 196, 79, 49, 203, 279, 61, 36, 69, 336, 284, 390, 281, 115, 269, 213, 295, 324, 382, 6, 117, 92, 294, 207, 163, 184, 103, 335, 177, 272, 301, 394, 342, 169, 162, 287, 87, 57, 373, 235, 114, 205, 399, 332, 64, 23, 273, 379, 225, 378, 119, 136, 296, 111, 381, 345, 277, 259, 359, 201, 10, 156, 374, 231, 105, 222, 145, 250, 108, 262, 385, 43, 141, 299, 362, 232, 58, 282, 223, 195, 285, 366, 327, 170, 290, 138, 356, 198, 50, 11, 71, 349, 206, 120, 346, 110, 347, 40, 319, 175, 21, 7, 102, 97, 364, 276, 384, 396, 181, 63, 28, 241, 202, 126, 340, 159, 174, 275, 65, 371, 370, 143, 271, 268, 167, 116, 100, 89, 140, 270, 393, 144, 306, 289, 150, 154]\n",
            "\n",
            " test_idx =  [348, 254, 227, 157, 182, 333, 380, 122, 112, 392, 226, 51, 367, 183, 123, 88, 190, 95, 224, 338, 9, 158, 18, 286, 32, 329, 83, 344, 358, 53, 240, 134, 302, 129, 330, 194, 161, 297, 343, 118, 98, 260, 388, 266, 267, 395, 72, 1, 321, 307, 168, 217, 204, 37, 171, 361, 0, 132, 31, 372, 56, 38, 187, 325, 237, 45, 82, 308, 263, 200, 383, 256, 165, 148, 24, 52, 377, 93, 278, 245, 251, 337, 193, 44, 121, 244, 363, 247, 30, 178, 339, 13, 236, 214, 368, 127, 151, 29, 73, 109, 68, 55, 211, 318, 12, 303, 197, 85, 46, 212, 258, 249, 305, 146, 76, 99, 326, 265, 274, 94]\n",
            "\n",
            "\n",
            " train_new_idx =  [153, 273, 144, 269, 161, 133, 192, 157, 234, 176, 6, 135, 139, 238, 250, 145, 103, 60, 93, 81, 201, 245, 167, 179, 98, 12, 27, 212, 187, 10, 15, 247, 279, 87, 76, 166, 119, 11, 183, 136, 97, 209, 233, 262, 159, 20, 18, 211, 46, 265, 0, 170, 8, 106, 68, 78, 89, 21, 240, 210, 216, 160, 235, 17, 271, 24, 107, 130, 116, 129, 71, 147, 101, 77, 83, 5, 100, 67, 19, 158, 172, 174, 203, 40, 168, 274, 104, 64, 70, 189, 91, 127, 50, 109, 88, 94, 110, 222, 123, 193, 3, 39, 215, 128, 33, 186, 23, 82, 246, 267, 72, 36, 231, 65, 131, 154, 241, 126, 224, 53, 169, 52, 257, 253, 206, 199, 28, 227, 84, 30, 114, 146, 155, 223, 45, 277, 248, 251, 74, 25, 184, 204, 200, 134, 85, 228, 111, 177, 117, 56, 16, 57, 278, 181, 102, 188, 132, 175, 140, 141, 202, 152, 59, 171, 205, 143, 190, 62, 263, 122, 258, 226, 105, 148, 198, 239, 49, 92, 137, 1, 260, 9, 236, 47, 69, 138, 261, 54, 120, 58, 26, 173, 112, 219, 197, 156, 51, 191, 259, 254, 182, 4, 142, 194, 124, 151, 32, 229, 268, 66, 115, 118, 255, 178, 163, 75, 270, 37, 2, 213, 249, 35, 272, 275, 242, 80, 149, 38, 208, 232, 90, 22, 243, 63, 99, 225, 14, 43, 44, 196, 218, 162, 214, 256, 180, 195, 48, 164, 237, 31, 230, 217]\n",
            "\n",
            " valid_idx =  [276, 95, 79, 7, 29, 165, 221, 150, 266, 73, 34, 86, 13, 264, 220, 244, 55, 185, 252, 113, 96, 207, 41, 125, 42, 61, 121, 108]\n",
            "total_length =  416\n",
            "batch[0].size() =  torch.Size([16, 3, 384, 384])\n",
            "batch[0].size() =  torch.Size([12, 3, 384, 384])\n",
            " ------------------ display images ------------------------\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-93c234046ca2>\u001b[0m in \u001b[0;36m<cell line: 195>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0mno_of_image_to_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_image_to_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_image_to_display\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m             projection_class, pkw = self._process_projection_requirements(\n\u001b[1;32m    756\u001b[0m                 *args, **kwargs)\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_originalPosition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0msubplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubplotSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_subplot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Width and height specified must be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m_from_subplot_args\u001b[0;34m(figure, args)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnargs_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subplot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1 or 3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_gridspec_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m_check_gridspec_exists\u001b[0;34m(figure, nrows, ncols)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# else gridspec not found:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         super().__init__(nrows, ncols,\n\u001b[0m\u001b[1;32m    380\u001b[0m                          \u001b[0mwidth_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                          height_ratios=height_ratios)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 f\"Number of rows must be a positive integer, not {nrows!r}\")\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     53\u001b[0m                 f\"Number of columns must be a positive integer, not {ncols!r}\")\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of columns must be a positive integer, not 5.0"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Websites for Binary classification on Images\n",
        "# https://github.com/jayrodge/Binary-Image-Classifier-PyTorch\n",
        "# https://github.com/jayrodge/Binary-Image-Classifier-PyTorch/blob/master/Binary_face_classifier.ipynb\n",
        "# https://hackernoon.com/binary-face-classifier-using-pytorch-2d835ccb7816 - this website explains the following code\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.utils.data as data\n",
        "# import torchvision.models as models\n",
        "# # from torch.autograd import Variable\n",
        "# from torchvision.models.vgg import model_urls\n",
        "# from torchviz import make_dot\n",
        "\n",
        "# The following program is obtained from:\n",
        "# https://github.com/jayrodge/Binary-Image-Classifier-PyTorch/blob/master/Binary_face_classifier.ipynb\n",
        "#\n",
        "# Visualization tool\n",
        "# https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch \n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#########################################################\n",
        "# Processing the Dataset\n",
        "#-------------------------\n",
        "# You might often need to process the image data before passing it to the model.\n",
        "# For example, if the sizes of all images are different (which often the case in large datasets), \n",
        "# then your model will throw an error. So resize them, and you can consider rescaling the pixel values also.\n",
        "# Apart from this, you can perform diverse transforms for data augmentation.\n",
        "# An elegant way to apply multiple transforms is using transform.compose() as shown below.\n",
        "#\n",
        "# When comes to loading/ preprocessing the data PyTorch is much simpler as compared to other libraries. \n",
        "# However, PyTorch has a built-in function called transforms using which you can perform all your pre-processing tasks \n",
        "##############################################################\n",
        "\n",
        "# how many samples per batch to load\n",
        "batch_size = 16\n",
        "\n",
        "# percentage of training set to use as validation\n",
        "test_size = 0.3 # test size is 30% of the total images that means training size is 70% of the total images\n",
        "valid_size = 0.1 # validation is 10% of the training image; validation comes before testing and validates each input\n",
        "\n",
        "\n",
        "ImageWidth=384\n",
        "ImageHeight=384\n",
        "\n",
        "#the most common choices for convolution filter kernel sizes appear to be square shape of sizes 3X3, 5X5\n",
        "#smaller sized kernel allows for more granular information. Here 5X5 is used.\n",
        "#Maxpool is needed when the image is larger. It keeps the item with the maximum value.\n",
        "convKernelSize1 =5\n",
        "convKernelSize2 =5\n",
        "maxpoolKernelSize = 2\n",
        "\n",
        "#conv1 Input Channel is defalut 3 for R, G, B channels directly from the colored Image\n",
        "convOutputChannel1 = 16\n",
        "#convInputChannel2 is the same as the convOutputChannel1\n",
        "convOutputChannel2 = 32\n",
        "linearOut1 = 256\n",
        "linearOut2 = 84\n",
        "\n",
        "dropOutValue = 0.2\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# for the first convolution and max pool\n",
        "height1=int((ImageHeight - convKernelSize1 + 1)/maxpoolKernelSize) \n",
        "width1=int((ImageWidth - convKernelSize1 + 1)/maxpoolKernelSize)\n",
        "\n",
        "# for the second convolution and max pool\n",
        "height2=int((height1 - convKernelSize2 + 1)/maxpoolKernelSize) \n",
        "width2=int((width1 - convKernelSize2 + 1)/maxpoolKernelSize)\n",
        "\n",
        "print(\" ImageWidth = \", ImageWidth)\n",
        "print(\" ImageHeight = \", ImageHeight)\n",
        "print(\" width1 = \", width1)\n",
        "print(\" height1 = \", height1)\n",
        "print(\" width2 = \", width2)\n",
        "print(\" height2 = \", height2)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.Resize(size=(ImageWidth,ImageHeight)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # need to keep these transforms, at least do resize all images to same size and co\n",
        "    ])\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "\n",
        "data_directory_face = '/content/gdrive/MyDrive/DNN_ML/face'\n",
        "\n",
        "print(\"\\n\\n ****************  START LOADING DATA AND PROCESSING ******************* \")\n",
        "\n",
        "###########\n",
        "# Select one of the above data_directory_XYZ\n",
        "# Depending on what you want to run. \n",
        "# You can change this part for different tests\n",
        "############\n",
        "\n",
        "data = datasets.ImageFolder(data_directory_face,transform=train_transform) \n",
        "# data = datasets.ImageFolder(data_directory_tumor)\n",
        "\n",
        "# The data needs to be split in Train, Test and validation set before training.\n",
        "# - Train set will be used to train the model.\n",
        "# - Validation set will be used for validating the model after each epoch. \n",
        "# - Test set will be used to evaluate the model once it is trained.\n",
        "\n",
        "\n",
        "num_data = len(data)\n",
        "print(\"num_data = \", num_data)\n",
        "\n",
        "indices_data = list(range(num_data))\n",
        "np.random.shuffle(indices_data)\n",
        "\n",
        "#For test and training\n",
        "#------------------------------\n",
        "split_tt = int(np.floor(test_size * num_data))\n",
        "train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt]\n",
        "\n",
        "print(\"\\n train_idx = \", train_idx)\n",
        "print(\"\\n test_idx = \", test_idx)\n",
        "\n",
        "#From Training separate something For validation (for each epoch)\n",
        "#---------------------------------------------------------------\n",
        "num_train = len(train_idx)\n",
        "indices_train = list(range(num_train))\n",
        "np.random.shuffle(indices_train)\n",
        "split_tv = int(np.floor(valid_size * num_train))\n",
        "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
        "\n",
        "print(\"\\n\\n train_new_idx = \", train_new_idx)\n",
        "print(\"\\n valid_idx = \", valid_idx)\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_new_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "\n",
        "\n",
        "# Loaders contains the data in tuple format (Image in form of tensor, label)\n",
        "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
        "\n",
        "# only running the data augmentation on the training data, double check if works\n",
        "valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, num_workers=1)\n",
        "\n",
        "# variable representing classes of the images\n",
        "classes = [0,1] # labeling either 0 (negative) or 1 (positive)\n",
        "\n",
        "total_length = len(test_loader)*batch_size + len(valid_loader)*batch_size + len(train_loader)*batch_size\n",
        "print(\"total_length = \", total_length)\n",
        "\n",
        "for batch in valid_loader:\n",
        "    print(\"batch[0].size() = \", batch[0].size())\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "\n",
        "print(\" ------------------ display images ------------------------\")\n",
        "# display some images with Labels to see if labeling appears correct\n",
        "\n",
        "\n",
        "no_of_image_to_display = 10\n",
        "for idx in np.arange(no_of_image_to_display):\n",
        "    ax = fig.add_subplot(2, no_of_image_to_display/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])\n",
        "\n",
        "display(plt.show())\n",
        "plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Building the Model: Decrypting the layers\n",
        "# ---------------------------------------------\n",
        "# The model class definition should always have __init__() method. In this, we will initialize the blocks/layers and other parameters.\n",
        "# Talking about the neural network layers, there are 3 main types in image classification: convolutional, max pooling, and dropout .\n",
        "\n",
        "# Convolution layers:  Convolutional layers will extract features from the input image and generate feature maps/activations. \n",
        "# You can decide how many activations you want using the filters argument. \n",
        "# Basically, when you apply convolution upon an image, the kernel will pass over the entire image in small parts, \n",
        "# and it will give an activation. It is demonstrated in the below image.\n",
        "\n",
        "# Pooling Layers\n",
        "# When we use conv2d layers, we end up with a lot of feature maps that occupy high computational space. \n",
        "# In order to decrease the computational time and space required, you can use Max pooling or average pooling.\n",
        "# For each patch/group of the map, only the maximum is chosen to form the output. \n",
        "# The below image clearly demonstrates the work.\n",
        "\n",
        "# Dropout Layers\n",
        "# You can guess its function from the name itself! It drops out like 10-20% of the data.\n",
        "# Overfitting is a common problem when you are training over the same data for many iterations/epochs. \n",
        "# By dropping out a randomly chosen 10% of data, the model will be able to generalize more to new data.\n",
        "# This concludes with a brief description of the layers we have used in our code. \n",
        "# Note that the final layer has output as 2, as it is binary classification."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
