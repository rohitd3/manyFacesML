{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torchviz\n",
        "!pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe_7DqAyJrCG",
        "outputId": "ddc3de74-91e3-49d0-f9a3-1b712c10facf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "restart_runtime()"
      ],
      "metadata": {
        "id": "HwCPldvIJsFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory_face = '/content/gdrive/MyDrive/DNN_ML/face'"
      ],
      "metadata": {
        "id": "WSDSIYZ05Ejl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_-2GoopJ8Nm",
        "outputId": "0310b5f5-e260-499d-8318-91988afd8f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory_face = '/content/gdrive/MyDrive/DNN_ML/face'"
      ],
      "metadata": {
        "id": "KmPgu7gb41f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############  These values can be changed for the Model and tested ############\n",
        "# Keep the Image width, height a multiple of 4.\n",
        "#This will ensure after Conv2D with 5X5 filter, and MaxPool(2,2) TWICE,\n",
        "# the values height2, width2 are still integers\n",
        "ImageWidth=384\n",
        "ImageHeight=384\n",
        "\n",
        "#the most common choices for convolution filter kernel sizes appear to be square shape of sizes 3X3, 5X5\n",
        "#smaller sized kernel allows for more granular information. Here 5X5 is used.\n",
        "#Maxpool is needed when the image is larger. It keeps the item with the maximum value.\n",
        "\n",
        "convKernelSize1 =5\n",
        "convKernelSize2 =5\n",
        "maxpoolKernelSize = 2\n",
        "\n",
        "#conv1 Input Channel is default 3 for R, G, B channels directly from the colored Image\n",
        "convOutputChannel1 = 16\n",
        "#convInputChannel2 is the same as the convOutputChannel1\n",
        "convOutputChannel2 = 32\n",
        "linearOut1 = 256\n",
        "linearOut2 = 84\n",
        "\n",
        "dropOutValue = 0.2\n",
        "\n",
        "# for the first convolution and max pool\n",
        "height1=int((ImageHeight - convKernelSize1 + 1)/maxpoolKernelSize) \n",
        "width1=int((ImageWidth - convKernelSize1 + 1)/maxpoolKernelSize)\n",
        "\n",
        "# for the second convolution and max pool\n",
        "height2=int((height1 - convKernelSize2 + 1)/maxpoolKernelSize) \n",
        "width2=int((width1 - convKernelSize2 + 1)/maxpoolKernelSize)\n",
        "\n",
        "print(\" ImageWidth = \", ImageWidth)\n",
        "print(\" ImageHeight = \", ImageHeight)\n",
        "print(\" width1 = \", width1)\n",
        "print(\" height1 = \", height1)\n",
        "print(\" width2 = \", width2)\n",
        "print(\" height2 = \", height2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEyaY45_KqJ5",
        "outputId": "280a900b-1b0d-4690-a660-de55e3d5afdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ImageWidth =  384\n",
            " ImageHeight =  384\n",
            " width1 =  190\n",
            " height1 =  190\n",
            " width2 =  93\n",
            " height2 =  93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 16\n",
        "# percentage of training set to use as validation\n",
        "test_size = 0.3\n",
        "valid_size = 0.1\n",
        "\n",
        "# Preprocessing steps\n",
        "# Horizontal Flip, Random Rotation, convert image array into PyTorch and normalize\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.Resize(size=(ImageWidth,ImageHeight)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # need to keep these transforms, at least do resize all images to same size and co\n",
        "    ])"
      ],
      "metadata": {
        "id": "ZZqXFbxaJ1Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.ImageFolder(data_directory_face,transform=train_transform) "
      ],
      "metadata": {
        "id": "KlmGMQtSKapf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data needs to be split in Train, Test and validation set before training.\n",
        " - Train set will be used to train the model.\n",
        " - Validation set will be used for validating the model after each epoch. \n",
        " - Test set will be used to evaluate the model once it is trained.\n"
      ],
      "metadata": {
        "id": "u0tMVYAjK6Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_data = len(data)\n",
        "print(\"num_data = \", num_data)\n",
        "\n",
        "indices_data = list(range(num_data))\n",
        "np.random.shuffle(indices_data)\n",
        "\n",
        "#For test and training\n",
        "\n",
        "split_tt = int(np.floor(test_size * num_data))\n",
        "\n",
        "# numpy.floor function operates element-wise on the input array, \n",
        "# returning a new array with the same shape as the input. \n",
        "# It rounds down each element of the input array to the nearest \n",
        "# integer that is less than or equal to that element\n",
        "\n",
        "train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt]\n",
        "\n",
        "print(\"\\n train_idx = \", train_idx)\n",
        "print(\"\\n test_idx = \", test_idx)\n",
        "\n",
        "#From training separate data For validation (for each epoch)\n",
        "num_train = len(train_idx)\n",
        "indices_train = list(range(num_train))\n",
        "np.random.shuffle(indices_train)\n",
        "split_tv = int(np.floor(valid_size * num_train))\n",
        "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
        "\n",
        "print(\"\\n train_new_idx = \", train_new_idx)\n",
        "print(\"\\n valid_idx = \", valid_idx)\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_new_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfLbKObSK5mz",
        "outputId": "3ed16fb8-de81-4217-a935-4dd7621bd2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_data =  400\n",
            "\n",
            " train_idx =  [327, 183, 171, 96, 7, 167, 379, 105, 236, 40, 339, 180, 189, 255, 120, 58, 302, 344, 276, 141, 215, 229, 103, 203, 313, 60, 224, 69, 73, 28, 375, 15, 247, 20, 230, 155, 25, 187, 243, 226, 118, 249, 383, 350, 377, 1, 13, 57, 237, 239, 235, 51, 131, 115, 86, 186, 316, 382, 309, 394, 271, 354, 304, 16, 365, 305, 52, 98, 100, 55, 84, 80, 88, 185, 280, 284, 90, 143, 328, 217, 110, 288, 374, 258, 0, 49, 47, 282, 210, 347, 97, 14, 146, 290, 125, 222, 368, 85, 194, 228, 72, 107, 67, 363, 182, 334, 42, 124, 298, 152, 209, 166, 323, 318, 232, 381, 59, 306, 397, 346, 380, 360, 77, 196, 176, 244, 211, 10, 181, 338, 256, 178, 387, 307, 193, 45, 351, 179, 63, 225, 35, 353, 61, 262, 385, 201, 398, 109, 311, 301, 285, 89, 3, 144, 293, 156, 75, 348, 219, 333, 366, 79, 299, 24, 34, 376, 221, 263, 372, 364, 159, 190, 204, 355, 340, 371, 62, 160, 33, 253, 4, 22, 9, 273, 30, 384, 8, 295, 134, 267, 18, 154, 342, 317, 321, 308, 242, 223, 112, 252, 238, 245, 289, 76, 199, 369, 195, 151, 248, 319, 343, 165, 121, 128, 279, 113, 297, 157, 87, 322, 92, 102, 216, 74, 150, 234, 325, 268, 241, 330, 148, 281, 170, 362, 32, 218, 114, 233, 214, 393, 188, 104, 349, 153, 251, 147, 132, 95, 2, 99, 231, 312, 310, 116, 46, 117, 300, 44, 39, 359, 212, 395, 27, 50, 261, 101, 174, 68, 391, 106, 388, 130, 303, 296, 213, 345, 259, 177, 149, 172]\n",
            "\n",
            " test_idx =  [335, 269, 37, 66, 133, 257, 314, 396, 21, 275, 336, 145, 370, 36, 41, 390, 220, 392, 119, 337, 17, 198, 83, 70, 137, 162, 329, 315, 205, 265, 331, 197, 332, 65, 191, 82, 361, 184, 254, 286, 163, 94, 111, 287, 260, 93, 138, 358, 71, 278, 126, 291, 208, 78, 139, 373, 43, 6, 135, 142, 192, 81, 272, 352, 250, 227, 140, 108, 324, 168, 11, 202, 386, 266, 91, 292, 207, 246, 53, 38, 56, 294, 161, 127, 341, 357, 54, 122, 173, 29, 129, 283, 164, 200, 356, 64, 31, 264, 19, 277, 206, 12, 378, 399, 240, 26, 326, 5, 175, 169, 123, 158, 367, 389, 23, 274, 136, 270, 48, 320]\n",
            "\n",
            " train_new_idx =  [27, 184, 45, 155, 253, 224, 60, 38, 210, 51, 101, 114, 228, 197, 66, 88, 132, 134, 245, 70, 26, 9, 256, 6, 83, 96, 15, 99, 78, 8, 29, 130, 79, 162, 274, 202, 154, 222, 231, 46, 193, 242, 180, 116, 263, 115, 212, 135, 213, 239, 269, 144, 247, 61, 216, 181, 138, 271, 150, 173, 221, 90, 227, 4, 262, 182, 112, 105, 74, 226, 204, 118, 62, 166, 240, 149, 31, 24, 254, 163, 236, 175, 148, 207, 174, 246, 95, 80, 142, 13, 36, 123, 252, 54, 117, 196, 108, 53, 100, 244, 50, 237, 48, 260, 35, 220, 267, 129, 32, 257, 232, 157, 97, 225, 223, 49, 84, 86, 199, 23, 5, 187, 2, 219, 268, 111, 110, 87, 183, 198, 72, 230, 47, 258, 122, 119, 161, 102, 272, 205, 11, 214, 133, 215, 77, 151, 264, 124, 200, 64, 278, 158, 137, 276, 25, 113, 176, 41, 172, 92, 22, 145, 12, 277, 188, 0, 44, 42, 164, 208, 34, 141, 251, 68, 194, 1, 211, 85, 143, 243, 209, 203, 63, 81, 106, 93, 171, 67, 233, 186, 10, 71, 160, 91, 241, 190, 21, 131, 279, 28, 7, 56, 43, 18, 192, 82, 250, 167, 19, 169, 147, 261, 191, 136, 103, 65, 58, 178, 98, 206, 37, 140, 270, 75, 275, 179, 195, 20, 234, 259, 89, 16, 107, 255, 126, 73, 265, 76, 17, 127, 189, 165, 156, 248, 52, 153, 3, 94, 249, 125, 69, 229]\n",
            "\n",
            " valid_idx =  [218, 30, 235, 33, 217, 120, 177, 159, 139, 185, 266, 55, 40, 128, 39, 170, 238, 201, 14, 146, 59, 109, 273, 121, 168, 57, 104, 152]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders contains the data in tuple format (Image in form of tensor, label)\n",
        "# train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1, transform=train_transform)\n",
        "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
        "\n",
        "# only running the data augmentation on the training data, double check if works\n",
        "valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, num_workers=1)\n",
        "\n",
        "# variable representing classes of the images\n",
        "classes = [0,1] # labeling either 0 (negative) or 1 (positive)\n",
        "\n",
        "total_length = len(test_loader)*batch_size + len(valid_loader)*batch_size + len(train_loader)*batch_size\n",
        "print(\"total_length = \", total_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT6iFP1lL8lC",
        "outputId": "021c9c10-24c2-46a9-ccbf-ce2db1099b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_length =  416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in valid_loader:\n",
        "    print(\"batch[0].size() = \", batch[0].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MekDp6XJMBOB",
        "outputId": "e8350057-eefc-4ec1-f9d8-a788a0c1dfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch[0].size() =  torch.Size([16, 3, 384, 384])\n",
            "batch[0].size() =  torch.Size([12, 3, 384, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DxekTfeGMNUt",
        "outputId": "33a422ab-47a5-4093-dc7f-65acf4b273de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_image_to_display = 16\n",
        "\n",
        "fig = plt.figure(figsize=(num_of_image_to_display, 4))\n",
        "\n",
        "for idx in np.arange(num_of_image_to_display):\n",
        "    ax = fig.add_subplot(2, int(num_of_image_to_display/2), idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])\n",
        "\n",
        "display(plt.show())\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Tgr9CgHQMWUO",
        "outputId": "07fc08d6-ac36-499f-d9bc-ab2c2c8d7d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-61107625c75e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_of_image_to_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_image_to_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_image_to_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "3LVDMdMENnBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is pytorch?\n",
        "PyTorch is an open-source machine learning library primarily used for deep learning tasks. It is developed by Facebook's AI Research Lab and provides a Python interface for building and training neural networks. PyTorch is known for its dynamic computational graph, which allows users to define and modify computational graphs on-the-fly during runtime.\n",
        "\n",
        "Neural network building blocks: PyTorch provides a rich set of pre-built modules and classes for building neural networks, such as various types of layers (e.g., fully connected, convolutional, recurrent), activation functions, loss functions, and optimizers.\n",
        "\n",
        "GPU acceleration: PyTorch seamlessly integrates with CUDA, a parallel computing platform, to leverage the power of NVIDIA GPUs for faster training and inference."
      ],
      "metadata": {
        "id": "MVjPmpA5WuMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Start training\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "print(\"train_on_gpu = \", train_on_gpu)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # define the two convolutional operations\n",
        "        # conv1 Input Channel =3 for the R, G, B channels directly from the Image\n",
        "        self.conv1 = nn.Conv2d(3, convOutputChannel1, convKernelSize1)\n",
        "        self.conv2 = nn.Conv2d(convOutputChannel1, convOutputChannel2, convKernelSize2)\n",
        "\n",
        "        # define the max pool operation\n",
        "        self.pool = nn.MaxPool2d(maxpoolKernelSize, maxpoolKernelSize)        \n",
        "        \n",
        "        self.dropout = nn.Dropout(dropOutValue)\n",
        "         \n",
        "        # define the Linear (fully connected) operations \n",
        "        self.fc1 = nn.Linear(convOutputChannel2* height2 * width2, linearOut1)\n",
        "        self.fc2 = nn.Linear(linearOut1, linearOut2)\n",
        "        self.fc3 = nn.Linear(linearOut2, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional, relu and max pooling layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # the linearization needs to start with = # of output channels * height2 * width2\n",
        "        x = x.view(-1, convOutputChannel2 * height2 * width2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bbp81quNewu",
        "outputId": "2f027d83-0c03-4188-9576-26e47f34f91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_on_gpu =  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        " Finally comes the training part. Here you need to decide two crucial things: Loss function and optimizer. \n",
        " There are various choices like SGD, Adam, etc.. for the optimizer.\n",
        " used Cross-Entropy loss, which is a popular choice in the case of classification problems.\n",
        " should also set a learning rate, which decides how fast your model learns."
      ],
      "metadata": {
        "id": "t4ohKSYQRxg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(\"++++++++++++ print the model ++++++++\")\n",
        "print(model)\n",
        "print(\"+++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hwQyRrqR5au",
        "outputId": "ad37d643-73c4-4127-f32b-aa864cc801d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++ print the model ++++++++\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=276768, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "+++++++++++++++++++++++++++++++++++++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.003\n",
        "\n",
        "# --------------- test with Stochastic gradient descent optimizer -------------------\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum= 0.9)"
      ],
      "metadata": {
        "id": "MPgUXMJoTHK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 5 # you may increase this number to train a final model\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss"
      ],
      "metadata": {
        "id": "UhwtujTOTU9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make_loss_graph function takes in data_list_val and data_list_train as input, which are lists of loss values for validation and training data. It plots these values on a graph and displays it."
      ],
      "metadata": {
        "id": "lk4TDARZ8y9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loss_graph(data_list_val, data_list_train):\n",
        "    plt.close()\n",
        "    plt.title(\"Training and Validation Loss per Epoch\")\n",
        "\n",
        "    val_label = \"Validation Loss \" \n",
        "    train_label = \"Training Loss \"\n",
        "    assert len(data_list_val) == len(data_list_train) ##makes sure the lengths of each are the same, will give error if not\n",
        "    length = len(data_list_train) - 1\n",
        "\n",
        "    plt.plot(data_list_val, label=val_label) ##validation data\n",
        "    plt.plot(data_list_train, label=train_label) ##training data\n",
        "    #plt.xlim((epoch_skip, length))\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.axis([0, 10, 0, 10])\n",
        "\n",
        "    display(plt.show()) ##displays plot\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "CCauM-AWTl_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Underfitting and overfitting\n",
        "\n",
        "Reasons for Underfitting:\n",
        "\n",
        "    High bias and low variance \n",
        "    The size of the training dataset used is not enough.\n",
        "    The model is too simple.\n",
        "    Training data is not cleaned and also contains noise in it.\n",
        "\n",
        "Techniques to reduce underfitting: \n",
        "\n",
        "    Increase model complexity\n",
        "    Increase the number of features, performing feature engineering\n",
        "    Remove noise from the data.\n",
        "    Increase the number of epochs or increase the duration of training to get better results.\n",
        "\n",
        "https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/#\n"
      ],
      "metadata": {
        "id": "YrYGJFVy8HpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_val_loss_all_epoch =[] \n",
        "list_train_loss_all_epoch =[]\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    print('####### EPOCH ', epoch)\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    train_items = 0.0\n",
        "    valid_items = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    print(' len(train_loader.dataset) = ', len(train_loader.dataset))\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        print('   train loss item = ', loss.item()*data.size(0))\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        train_items +=1\n",
        "        \n",
        "    ######################    \n",
        "    # validate or evaluate the model #\n",
        "    # To evaluate the model, it should be changed from model.train() to model.eval()\n",
        "    ######################\n",
        "    model.eval()\n",
        "    print(' len(valid_loader.dataset) = ', len(valid_loader.dataset))\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        print('   valid loss item = ', loss.item()*data.size(0))\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        valid_items +=1\n",
        "    \n",
        "    # calculate average losses\n",
        "    print('\\n train_loss = ', train_loss)\n",
        "    print(' valid_loss = ', valid_loss)\n",
        "    print('\\n train_items = ', train_items)\n",
        "    print(' valid_items = ', valid_items)\n",
        "    #train_loss = train_loss/len(train_loader.dataset) # incorrect: this was averaging over 500 (the total amount of images available)\n",
        "    #valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    train_loss = train_loss/train_items # this is averaging correctly\n",
        "    valid_loss = valid_loss/valid_items\n",
        "    print('\\n average train_loss = ', train_loss)\n",
        "    print(' average valid_loss = ', valid_loss)\n",
        "\n",
        "    list_val_loss_all_epoch.append(valid_loss)\n",
        "    list_train_loss_all_epoch.append(train_loss)\n",
        "\n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\t Average Training Loss: {:.6f} \\t Average Validation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "\n",
        "\n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "make_loss_graph(list_val_loss_all_epoch, list_train_loss_all_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z6UJFlfBTseR",
        "outputId": "f984fc82-8edf-49f7-a02c-c2fcd9774368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####### EPOCH  1\n",
            " len(train_loader.dataset) =  400\n",
            "   train loss item =  0.024645444005727768\n",
            "   train loss item =  0.32031652331352234\n",
            "   train loss item =  0.05283474177122116\n",
            "   train loss item =  0.9114115238189697\n",
            "   train loss item =  0.04939626529812813\n",
            "   train loss item =  0.03176403045654297\n",
            "   train loss item =  0.02037661150097847\n",
            "   train loss item =  0.11796353757381439\n",
            "   train loss item =  0.4087546467781067\n",
            "   train loss item =  0.12971322238445282\n",
            "   train loss item =  0.2309323251247406\n",
            "   train loss item =  0.17112958431243896\n",
            "   train loss item =  0.00805441290140152\n",
            "   train loss item =  0.7278347015380859\n",
            "   train loss item =  0.03808647766709328\n",
            "   train loss item =  0.001081527181668207\n",
            " len(valid_loader.dataset) =  400\n",
            "   valid loss item =  0.3364920914173126\n",
            "   valid loss item =  0.7835705280303955\n",
            "\n",
            " train_loss =  3.244295575626893\n",
            " valid_loss =  1.1200626194477081\n",
            "\n",
            " train_items =  16.0\n",
            " valid_items =  2.0\n",
            "\n",
            " average train_loss =  0.2027684734766808\n",
            " average valid_loss =  0.5600313097238541\n",
            "Epoch: 1 \t Average Training Loss: 0.202768 \t Average Validation Loss: 0.560031\n",
            "Validation loss decreased (1.058500 --> 0.560031).  Saving model ...\n",
            "####### EPOCH  2\n",
            " len(train_loader.dataset) =  400\n",
            "   train loss item =  0.017503857612609863\n",
            "   train loss item =  0.012020493857562542\n",
            "   train loss item =  1.2742114067077637\n",
            "   train loss item =  0.02505093812942505\n",
            "   train loss item =  0.03209812566637993\n",
            "   train loss item =  0.16146953403949738\n",
            "   train loss item =  0.1393091380596161\n",
            "   train loss item =  1.8395599126815796\n",
            "   train loss item =  6.752047061920166\n",
            "   train loss item =  0.016535572707653046\n",
            "   train loss item =  0.012823164463043213\n",
            "   train loss item =  0.04886973649263382\n",
            "   train loss item =  0.11911410838365555\n",
            "   train loss item =  0.8541668057441711\n",
            "   train loss item =  8.914411544799805\n",
            "   train loss item =  0.007184454007074237\n",
            " len(valid_loader.dataset) =  400\n",
            "   valid loss item =  5.69299840927124\n",
            "   valid loss item =  0.004451676271855831\n",
            "\n",
            " train_loss =  20.226375855272636\n",
            " valid_loss =  5.697450085543096\n",
            "\n",
            " train_items =  16.0\n",
            " valid_items =  2.0\n",
            "\n",
            " average train_loss =  1.2641484909545397\n",
            " average valid_loss =  2.848725042771548\n",
            "Epoch: 2 \t Average Training Loss: 1.264148 \t Average Validation Loss: 2.848725\n",
            "####### EPOCH  3\n",
            " len(train_loader.dataset) =  400\n",
            "   train loss item =  0.28011396527290344\n",
            "   train loss item =  0.23054836690425873\n",
            "   train loss item =  0.05393335595726967\n",
            "   train loss item =  1.2450270652770996\n",
            "   train loss item =  0.08126480132341385\n",
            "   train loss item =  0.5129839181900024\n",
            "   train loss item =  0.4966256022453308\n",
            "   train loss item =  0.08642233908176422\n",
            "   train loss item =  1.2715575695037842\n",
            "   train loss item =  0.1827414333820343\n",
            "   train loss item =  0.20200985670089722\n",
            "   train loss item =  0.2556696832180023\n",
            "   train loss item =  0.10684928297996521\n",
            "   train loss item =  0.11533006280660629\n",
            "   train loss item =  0.12136797606945038\n",
            "   train loss item =  0.1404786966741085\n",
            " len(valid_loader.dataset) =  400\n",
            "   valid loss item =  0.19773808121681213\n",
            "   valid loss item =  0.9970040023326874\n",
            "\n",
            " train_loss =  5.382923975586891\n",
            " valid_loss =  1.1947420835494995\n",
            "\n",
            " train_items =  16.0\n",
            " valid_items =  2.0\n",
            "\n",
            " average train_loss =  0.3364327484741807\n",
            " average valid_loss =  0.5973710417747498\n",
            "Epoch: 3 \t Average Training Loss: 0.336433 \t Average Validation Loss: 0.597371\n",
            "####### EPOCH  4\n",
            " len(train_loader.dataset) =  400\n",
            "   train loss item =  0.20712696015834808\n",
            "   train loss item =  0.07195495814085007\n",
            "   train loss item =  0.14663034677505493\n",
            "   train loss item =  0.1379862129688263\n",
            "   train loss item =  0.11275184154510498\n",
            "   train loss item =  0.16099438071250916\n",
            "   train loss item =  0.08503907173871994\n",
            "   train loss item =  0.0856439545750618\n",
            "   train loss item =  0.29068124294281006\n",
            "   train loss item =  0.017780523747205734\n",
            "   train loss item =  0.05050569772720337\n",
            "   train loss item =  0.011246606707572937\n",
            "   train loss item =  0.0094155790284276\n",
            "   train loss item =  0.059190504252910614\n",
            "   train loss item =  0.027335597202181816\n",
            "   train loss item =  0.02183528244495392\n",
            " len(valid_loader.dataset) =  400\n",
            "   valid loss item =  0.02724332921206951\n",
            "   valid loss item =  0.4153520464897156\n",
            "\n",
            " train_loss =  1.4961187606677413\n",
            " valid_loss =  0.4425953757017851\n",
            "\n",
            " train_items =  16.0\n",
            " valid_items =  2.0\n",
            "\n",
            " average train_loss =  0.09350742254173383\n",
            " average valid_loss =  0.22129768785089254\n",
            "Epoch: 4 \t Average Training Loss: 0.093507 \t Average Validation Loss: 0.221298\n",
            "Validation loss decreased (0.560031 --> 0.221298).  Saving model ...\n",
            "####### EPOCH  5\n",
            " len(train_loader.dataset) =  400\n",
            "   train loss item =  0.29646751284599304\n",
            "   train loss item =  0.06760089844465256\n",
            "   train loss item =  0.07073017954826355\n",
            "   train loss item =  0.053998708724975586\n",
            "   train loss item =  0.005736634135246277\n",
            "   train loss item =  0.0050850375555455685\n",
            "   train loss item =  0.0208502858877182\n",
            "   train loss item =  0.246884286403656\n",
            "   train loss item =  0.015171251259744167\n",
            "   train loss item =  0.02811436913907528\n",
            "   train loss item =  0.003049638122320175\n",
            "   train loss item =  0.001593883614987135\n",
            "   train loss item =  0.001167911570519209\n",
            "   train loss item =  0.1199827566742897\n",
            "   train loss item =  0.02852724678814411\n",
            "   train loss item =  1.006018728017807\n",
            " len(valid_loader.dataset) =  400\n",
            "   valid loss item =  0.4509490430355072\n",
            "   valid loss item =  0.032683937810361385\n",
            "\n",
            " train_loss =  1.9709793287329376\n",
            " valid_loss =  0.4836329808458686\n",
            "\n",
            " train_items =  16.0\n",
            " valid_items =  2.0\n",
            "\n",
            " average train_loss =  0.1231862080458086\n",
            " average valid_loss =  0.2418164904229343\n",
            "Epoch: 5 \t Average Training Loss: 0.123186 \t Average Validation Loss: 0.241816\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# Analysis of the Model\n",
        "# plotting loss and accuracy over epochs to see how it changed over training\n",
        "######################################################\n",
        "print(\"\\n ****************  START TEST ******************* \")\n",
        "# plot the accuracy\n",
        "# confusion matrix\n",
        "# dont train for 700 epochs - do for less ~20\n",
        "\n",
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "\n",
        "model.eval()\n",
        "i=1\n",
        "# iterate over test data\n",
        "len(test_loader)\n",
        "for data, target in test_loader:\n",
        "    i=i+1\n",
        "    if len(target)!=batch_size:\n",
        "        continue\n",
        "        \n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "#     print(target)\n",
        "    \n",
        "    for i in range(batch_size):       \n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(2):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zgOFcN4UXWF",
        "outputId": "a6e8fa5b-b705-49c1-8ff3-f639f1119967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ****************  START TEST ******************* \n",
            "Test Loss: 0.030040\n",
            "\n",
            "Test Accuracy of     0: 95% (57/60)\n",
            "Test Accuracy of     1: 94% (49/52)\n",
            "\n",
            "Test Accuracy (Overall): 94% (106/112)\n"
          ]
        }
      ]
    }
  ]
}