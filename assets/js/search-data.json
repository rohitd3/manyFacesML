{
  
    
        "post0": {
            "title": "Identifying faces",
            "content": "!pip install torch torchvision !pip install torchviz !pip install --upgrade torch torchvision # the Pandas mismatch hasn&#39;t been a problem, because we aren&#39;t using it in our code # ERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version &gt;= &quot;3.0&quot;, but you&#39;ll have pandas 1.1.0 which is incompatible. . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1) Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2) Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99) Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99) Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101) Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96) Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66) Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58) Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91) Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1) Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91) Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3) Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch) (67.7.2) Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch) (0.40.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (16.0.3) Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4) Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1) Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (1.26.15) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2022.12.7) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (3.4) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0) Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2) Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1.2) Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.7.99) Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.7.99) Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.7.101) Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (8.5.0.96) Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.10.3.66) Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (10.9.0.58) Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (10.2.10.91) Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.4.0.1) Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.7.4.91) Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2.14.3) Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (11.7.91) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch-&gt;torchviz) (67.7.2) Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch-&gt;torchviz) (0.40.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;torchviz) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;torchviz) (16.0.3) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-&gt;torchviz) (2.1.2) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch-&gt;torchviz) (1.3.0) Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1) Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2) Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99) Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99) Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101) Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96) Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66) Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58) Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91) Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1) Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91) Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3) Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch) (67.7.2) Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch) (0.40.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (16.0.3) Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4) Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1) Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (1.26.15) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2022.12.7) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (3.4) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0) . import os def restart_runtime(): os.kill(os.getpid(), 9) restart_runtime() . # Websites for Binary classification on Images # https://github.com/jayrodge/Binary-Image-Classifier-PyTorch # https://github.com/jayrodge/Binary-Image-Classifier-PyTorch/blob/master/Binary_face_classifier.ipynb # https://hackernoon.com/binary-face-classifier-using-pytorch-2d835ccb7816 - this website explains the following code import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms from torch.utils.data.sampler import SubsetRandomSampler # import torch.nn as nn # import torch.optim as optim # import torch.utils.data as data # import torchvision.models as models # # from torch.autograd import Variable # from torchvision.models.vgg import model_urls # from torchviz import make_dot # The following program is obtained from: # https://github.com/jayrodge/Binary-Image-Classifier-PyTorch/blob/master/Binary_face_classifier.ipynb # # Visualization tool # https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch from google.colab import drive from google.colab import files from PIL import Image drive.mount(&#39;/content/gdrive&#39;) ######################################################### # Processing the Dataset #- # You might often need to process the image data before passing it to the model. # For example, if the sizes of all images are different (which often the case in large datasets), # then your model will throw an error. So resize them, and you can consider rescaling the pixel values also. # Apart from this, you can perform diverse transforms for data augmentation. # An elegant way to apply multiple transforms is using transform.compose() as shown below. # # When comes to loading/ preprocessing the data PyTorch is much simpler as compared to other libraries. # However, PyTorch has a built-in function called transforms using which you can perform all your pre-processing tasks ############################################################## # how many samples per batch to load batch_size = 16 # percentage of training set to use as validation test_size = 0.3 # test size is 30% of the total images that means training size is 70% of the total images valid_size = 0.1 # validation is 10% of the training image; validation comes before testing and validates each input ImageWidth=384 ImageHeight=384 #the most common choices for convolution filter kernel sizes appear to be square shape of sizes 3X3, 5X5 #smaller sized kernel allows for more granular information. Here 5X5 is used. #Maxpool is needed when the image is larger. It keeps the item with the maximum value. convKernelSize1 =5 convKernelSize2 =5 maxpoolKernelSize = 2 #conv1 Input Channel is defalut 3 for R, G, B channels directly from the colored Image convOutputChannel1 = 16 #convInputChannel2 is the same as the convOutputChannel1 convOutputChannel2 = 32 linearOut1 = 256 linearOut2 = 84 dropOutValue = 0.2 # # for the first convolution and max pool height1=int((ImageHeight - convKernelSize1 + 1)/maxpoolKernelSize) width1=int((ImageWidth - convKernelSize1 + 1)/maxpoolKernelSize) # for the second convolution and max pool height2=int((height1 - convKernelSize2 + 1)/maxpoolKernelSize) width2=int((width1 - convKernelSize2 + 1)/maxpoolKernelSize) print(&quot; ImageWidth = &quot;, ImageWidth) print(&quot; ImageHeight = &quot;, ImageHeight) print(&quot; width1 = &quot;, width1) print(&quot; height1 = &quot;, height1) print(&quot; width2 = &quot;, width2) print(&quot; height2 = &quot;, height2) train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomRotation(10), transforms.Resize(size=(ImageWidth,ImageHeight)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # need to keep these transforms, at least do resize all images to same size and co ]) ########################################################################### # data_directory_face = &#39;/content/gdrive/MyDrive/DNN_ML/face&#39; print(&quot; n n **************** START LOADING DATA AND PROCESSING ******************* &quot;) ########### # Select one of the above data_directory_XYZ # Depending on what you want to run. # You can change this part for different tests ############ data = datasets.ImageFolder(data_directory_face,transform=train_transform) # data = datasets.ImageFolder(data_directory_tumor) # The data needs to be split in Train, Test and validation set before training. # - Train set will be used to train the model. # - Validation set will be used for validating the model after each epoch. # - Test set will be used to evaluate the model once it is trained. num_data = len(data) print(&quot;num_data = &quot;, num_data) indices_data = list(range(num_data)) np.random.shuffle(indices_data) #For test and training # split_tt = int(np.floor(test_size * num_data)) train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt] print(&quot; n train_idx = &quot;, train_idx) print(&quot; n test_idx = &quot;, test_idx) #From Training separate something For validation (for each epoch) # num_train = len(train_idx) indices_train = list(range(num_train)) np.random.shuffle(indices_train) split_tv = int(np.floor(valid_size * num_train)) train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv] print(&quot; n n train_new_idx = &quot;, train_new_idx) print(&quot; n valid_idx = &quot;, valid_idx) # define samplers for obtaining training and validation batches train_sampler = SubsetRandomSampler(train_new_idx) valid_sampler = SubsetRandomSampler(valid_idx) test_sampler = SubsetRandomSampler(test_idx) # Loaders contains the data in tuple format (Image in form of tensor, label) train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1) # only running the data augmentation on the training data, double check if works valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=valid_sampler, num_workers=1) test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, num_workers=1) # variable representing classes of the images classes = [0,1] # labeling either 0 (negative) or 1 (positive) total_length = len(test_loader)*batch_size + len(valid_loader)*batch_size + len(train_loader)*batch_size print(&quot;total_length = &quot;, total_length) for batch in valid_loader: print(&quot;batch[0].size() = &quot;, batch[0].size()) import matplotlib.pyplot as plt %matplotlib inline # helper function to un-normalize and display an image def imshow(img): img = img / 2 + 0.5 # unnormalize plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # obtain one batch of training images dataiter = iter(train_loader) images, labels = next(dataiter) images = images.numpy() # convert images to numpy for display # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(10, 4)) print(&quot; display images &quot;) # display some images with Labels to see if labeling appears correct no_of_image_to_display = 10 for idx in np.arange(no_of_image_to_display): ax = fig.add_subplot(2, no_of_image_to_display/2, idx+1, xticks=[], yticks=[]) imshow(images[idx]) ax.set_title(classes[labels[idx]]) display(plt.show()) plt.close() # Building the Model: Decrypting the layers # # The model class definition should always have __init__() method. In this, we will initialize the blocks/layers and other parameters. # Talking about the neural network layers, there are 3 main types in image classification: convolutional, max pooling, and dropout . # Convolution layers: Convolutional layers will extract features from the input image and generate feature maps/activations. # You can decide how many activations you want using the filters argument. # Basically, when you apply convolution upon an image, the kernel will pass over the entire image in small parts, # and it will give an activation. It is demonstrated in the below image. # Pooling Layers # When we use conv2d layers, we end up with a lot of feature maps that occupy high computational space. # In order to decrease the computational time and space required, you can use Max pooling or average pooling. # For each patch/group of the map, only the maximum is chosen to form the output. # The below image clearly demonstrates the work. # Dropout Layers # You can guess its function from the name itself! It drops out like 10-20% of the data. # Overfitting is a common problem when you are training over the same data for many iterations/epochs. # By dropping out a randomly chosen 10% of data, the model will be able to generalize more to new data. # This concludes with a brief description of the layers we have used in our code. # Note that the final layer has output as 2, as it is binary classification. . Mounted at /content/gdrive ImageWidth = 384 ImageHeight = 384 width1 = 190 height1 = 190 width2 = 93 height2 = 93 **************** START LOADING DATA AND PROCESSING ******************* num_data = 400 train_idx = [233, 261, 54, 14, 246, 47, 309, 360, 215, 189, 131, 22, 252, 25, 186, 164, 66, 253, 351, 84, 33, 280, 155, 334, 133, 137, 239, 90, 3, 288, 219, 106, 160, 234, 313, 210, 172, 220, 74, 180, 173, 248, 391, 389, 8, 35, 70, 62, 130, 369, 39, 107, 328, 376, 48, 96, 310, 312, 316, 78, 322, 135, 16, 81, 147, 355, 341, 191, 365, 42, 257, 188, 176, 283, 317, 128, 352, 153, 41, 353, 386, 15, 304, 17, 20, 179, 2, 238, 320, 91, 292, 77, 113, 354, 331, 291, 300, 27, 101, 152, 4, 229, 80, 242, 230, 86, 209, 255, 357, 208, 34, 75, 104, 5, 375, 216, 293, 221, 166, 398, 139, 149, 314, 59, 199, 350, 387, 228, 323, 125, 298, 315, 264, 67, 60, 397, 26, 218, 142, 19, 124, 192, 185, 243, 311, 196, 79, 49, 203, 279, 61, 36, 69, 336, 284, 390, 281, 115, 269, 213, 295, 324, 382, 6, 117, 92, 294, 207, 163, 184, 103, 335, 177, 272, 301, 394, 342, 169, 162, 287, 87, 57, 373, 235, 114, 205, 399, 332, 64, 23, 273, 379, 225, 378, 119, 136, 296, 111, 381, 345, 277, 259, 359, 201, 10, 156, 374, 231, 105, 222, 145, 250, 108, 262, 385, 43, 141, 299, 362, 232, 58, 282, 223, 195, 285, 366, 327, 170, 290, 138, 356, 198, 50, 11, 71, 349, 206, 120, 346, 110, 347, 40, 319, 175, 21, 7, 102, 97, 364, 276, 384, 396, 181, 63, 28, 241, 202, 126, 340, 159, 174, 275, 65, 371, 370, 143, 271, 268, 167, 116, 100, 89, 140, 270, 393, 144, 306, 289, 150, 154] test_idx = [348, 254, 227, 157, 182, 333, 380, 122, 112, 392, 226, 51, 367, 183, 123, 88, 190, 95, 224, 338, 9, 158, 18, 286, 32, 329, 83, 344, 358, 53, 240, 134, 302, 129, 330, 194, 161, 297, 343, 118, 98, 260, 388, 266, 267, 395, 72, 1, 321, 307, 168, 217, 204, 37, 171, 361, 0, 132, 31, 372, 56, 38, 187, 325, 237, 45, 82, 308, 263, 200, 383, 256, 165, 148, 24, 52, 377, 93, 278, 245, 251, 337, 193, 44, 121, 244, 363, 247, 30, 178, 339, 13, 236, 214, 368, 127, 151, 29, 73, 109, 68, 55, 211, 318, 12, 303, 197, 85, 46, 212, 258, 249, 305, 146, 76, 99, 326, 265, 274, 94] train_new_idx = [153, 273, 144, 269, 161, 133, 192, 157, 234, 176, 6, 135, 139, 238, 250, 145, 103, 60, 93, 81, 201, 245, 167, 179, 98, 12, 27, 212, 187, 10, 15, 247, 279, 87, 76, 166, 119, 11, 183, 136, 97, 209, 233, 262, 159, 20, 18, 211, 46, 265, 0, 170, 8, 106, 68, 78, 89, 21, 240, 210, 216, 160, 235, 17, 271, 24, 107, 130, 116, 129, 71, 147, 101, 77, 83, 5, 100, 67, 19, 158, 172, 174, 203, 40, 168, 274, 104, 64, 70, 189, 91, 127, 50, 109, 88, 94, 110, 222, 123, 193, 3, 39, 215, 128, 33, 186, 23, 82, 246, 267, 72, 36, 231, 65, 131, 154, 241, 126, 224, 53, 169, 52, 257, 253, 206, 199, 28, 227, 84, 30, 114, 146, 155, 223, 45, 277, 248, 251, 74, 25, 184, 204, 200, 134, 85, 228, 111, 177, 117, 56, 16, 57, 278, 181, 102, 188, 132, 175, 140, 141, 202, 152, 59, 171, 205, 143, 190, 62, 263, 122, 258, 226, 105, 148, 198, 239, 49, 92, 137, 1, 260, 9, 236, 47, 69, 138, 261, 54, 120, 58, 26, 173, 112, 219, 197, 156, 51, 191, 259, 254, 182, 4, 142, 194, 124, 151, 32, 229, 268, 66, 115, 118, 255, 178, 163, 75, 270, 37, 2, 213, 249, 35, 272, 275, 242, 80, 149, 38, 208, 232, 90, 22, 243, 63, 99, 225, 14, 43, 44, 196, 218, 162, 214, 256, 180, 195, 48, 164, 237, 31, 230, 217] valid_idx = [276, 95, 79, 7, 29, 165, 221, 150, 266, 73, 34, 86, 13, 264, 220, 244, 55, 185, 252, 113, 96, 207, 41, 125, 42, 61, 121, 108] total_length = 416 batch[0].size() = torch.Size([16, 3, 384, 384]) batch[0].size() = torch.Size([12, 3, 384, 384]) display images . ValueError Traceback (most recent call last) &lt;ipython-input-1-93c234046ca2&gt; in &lt;cell line: 195&gt;() 194 no_of_image_to_display = 10 195 for idx in np.arange(no_of_image_to_display): --&gt; 196 ax = fig.add_subplot(2, no_of_image_to_display/2, idx+1, xticks=[], yticks=[]) 197 imshow(images[idx]) 198 ax.set_title(classes[labels[idx]]) /usr/local/lib/python3.10/dist-packages/matplotlib/figure.py in add_subplot(self, *args, **kwargs) 755 projection_class, pkw = self._process_projection_requirements( 756 *args, **kwargs) --&gt; 757 ax = projection_class(self, *args, **pkw) 758 key = (projection_class, pkw) 759 return self._add_axes_internal(ax, key) /usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py in __init__(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs) 642 else: 643 self._position = self._originalPosition = mtransforms.Bbox.unit() --&gt; 644 subplotspec = SubplotSpec._from_subplot_args(fig, args) 645 if self._position.width &lt; 0 or self._position.height &lt; 0: 646 raise ValueError(&#39;Width and height specified must be non-negative&#39;) /usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py in _from_subplot_args(figure, args) 585 raise _api.nargs_error(&#34;subplot&#34;, takes=&#34;1 or 3&#34;, given=len(args)) 586 --&gt; 587 gs = GridSpec._check_gridspec_exists(figure, rows, cols) 588 if gs is None: 589 gs = GridSpec(rows, cols, figure=figure) /usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py in _check_gridspec_exists(figure, nrows, ncols) 224 return gs 225 # else gridspec not found: --&gt; 226 return GridSpec(nrows, ncols, figure=figure) 227 228 def __getitem__(self, key): /usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py in __init__(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios) 377 self.figure = figure 378 --&gt; 379 super().__init__(nrows, ncols, 380 width_ratios=width_ratios, 381 height_ratios=height_ratios) /usr/local/lib/python3.10/dist-packages/matplotlib/gridspec.py in __init__(self, nrows, ncols, height_ratios, width_ratios) 50 f&#34;Number of rows must be a positive integer, not {nrows!r}&#34;) 51 if not isinstance(ncols, Integral) or ncols &lt;= 0: &gt; 52 raise ValueError( 53 f&#34;Number of columns must be a positive integer, not {ncols!r}&#34;) 54 self._nrows, self._ncols = nrows, ncols ValueError: Number of columns must be a positive integer, not 5.0 . &lt;Figure size 1000x400 with 0 Axes&gt; .",
            "url": "https://rohitd3.github.io/manyFacesML/python/2023/05/20/facesIdentify.html",
            "relUrl": "/python/2023/05/20/facesIdentify.html",
            "date": " • May 20, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "What is pytorch?",
            "content": "!pip install torch torchvision !pip install torchviz !pip install --upgrade torch torchvision . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118) Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (16.0.5) Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4) Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1) Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (1.26.15) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2022.12.7) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (3.4) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0) Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting torchviz Downloading torchviz-0.0.2.tar.gz (4.9 kB) Preparing metadata (setup.py) ... done Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118) Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (3.1.2) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-&gt;torchviz) (2.0.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;torchviz) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch-&gt;torchviz) (16.0.5) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-&gt;torchviz) (2.1.2) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch-&gt;torchviz) (1.3.0) Building wheels for collected packages: torchviz Building wheel for torchviz (setup.py) ... done Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=4c119a47dbce7a5e57876f43ed837b694247e5bfd2179533a9d9a3406461351b Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094 Successfully built torchviz Installing collected packages: torchviz Successfully installed torchviz-0.0.2 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118) Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2) Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0) Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (3.25.2) Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch) (16.0.5) Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4) Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1) Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (1.26.15) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2022.12.7) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torchvision) (3.4) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0) . import os def restart_runtime(): os.kill(os.getpid(), 9) restart_runtime() . from google.colab import drive from google.colab import files from PIL import Image drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . data_directory_face = &#39;/content/gdrive/MyDrive/DNN_ML/face&#39; . # Keep the Image width, height a multiple of 4. #This will ensure after Conv2D with 5X5 filter, and MaxPool(2,2) TWICE, # the values height2, width2 are still integers ImageWidth=384 ImageHeight=384 #the most common choices for convolution filter kernel sizes appear to be square shape of sizes 3X3, 5X5 #smaller sized kernel allows for more granular information. Here 5X5 is used. #Maxpool is needed when the image is larger. It keeps the item with the maximum value. convKernelSize1 =5 convKernelSize2 =5 maxpoolKernelSize = 2 #conv1 Input Channel is default 3 for R, G, B channels directly from the colored Image convOutputChannel1 = 16 #convInputChannel2 is the same as the convOutputChannel1 convOutputChannel2 = 32 linearOut1 = 256 linearOut2 = 84 dropOutValue = 0.2 # for the first convolution and max pool height1=int((ImageHeight - convKernelSize1 + 1)/maxpoolKernelSize) width1=int((ImageWidth - convKernelSize1 + 1)/maxpoolKernelSize) # for the second convolution and max pool height2=int((height1 - convKernelSize2 + 1)/maxpoolKernelSize) width2=int((width1 - convKernelSize2 + 1)/maxpoolKernelSize) print(&quot; ImageWidth = &quot;, ImageWidth) print(&quot; ImageHeight = &quot;, ImageHeight) print(&quot; width1 = &quot;, width1) print(&quot; height1 = &quot;, height1) print(&quot; width2 = &quot;, width2) print(&quot; height2 = &quot;, height2) . ImageWidth = 384 ImageHeight = 384 width1 = 190 height1 = 190 width2 = 93 height2 = 93 . batch_size = 16 # percentage of training set to use as validation test_size = 0.3 valid_size = 0.1 # Preprocessing steps # Horizontal Flip, Random Rotation, convert image array into PyTorch and normalize train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomRotation(10), transforms.Resize(size=(ImageWidth,ImageHeight)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # need to keep these transforms, at least do resize all images to same size and co ]) . data = datasets.ImageFolder(data_directory_face,transform=train_transform) . The data needs to be split in Train, Test and validation set before training. . Train set will be used to train the model. | Validation set will be used for validating the model after each epoch. | Test set will be used to evaluate the model once it is trained. | . num_data = len(data) print(&quot;num_data = &quot;, num_data) indices_data = list(range(num_data)) np.random.shuffle(indices_data) #For test and training split_tt = int(np.floor(test_size * num_data)) # numpy.floor function operates element-wise on the input array, # returning a new array with the same shape as the input. # It rounds down each element of the input array to the nearest # integer that is less than or equal to that element train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt] print(&quot; n train_idx = &quot;, train_idx) print(&quot; n test_idx = &quot;, test_idx) #From training separate data For validation (for each epoch) num_train = len(train_idx) indices_train = list(range(num_train)) np.random.shuffle(indices_train) split_tv = int(np.floor(valid_size * num_train)) train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv] print(&quot; n train_new_idx = &quot;, train_new_idx) print(&quot; n valid_idx = &quot;, valid_idx) # define samplers for obtaining training and validation batches train_sampler = SubsetRandomSampler(train_new_idx) valid_sampler = SubsetRandomSampler(valid_idx) test_sampler = SubsetRandomSampler(test_idx) . num_data = 400 train_idx = [327, 183, 171, 96, 7, 167, 379, 105, 236, 40, 339, 180, 189, 255, 120, 58, 302, 344, 276, 141, 215, 229, 103, 203, 313, 60, 224, 69, 73, 28, 375, 15, 247, 20, 230, 155, 25, 187, 243, 226, 118, 249, 383, 350, 377, 1, 13, 57, 237, 239, 235, 51, 131, 115, 86, 186, 316, 382, 309, 394, 271, 354, 304, 16, 365, 305, 52, 98, 100, 55, 84, 80, 88, 185, 280, 284, 90, 143, 328, 217, 110, 288, 374, 258, 0, 49, 47, 282, 210, 347, 97, 14, 146, 290, 125, 222, 368, 85, 194, 228, 72, 107, 67, 363, 182, 334, 42, 124, 298, 152, 209, 166, 323, 318, 232, 381, 59, 306, 397, 346, 380, 360, 77, 196, 176, 244, 211, 10, 181, 338, 256, 178, 387, 307, 193, 45, 351, 179, 63, 225, 35, 353, 61, 262, 385, 201, 398, 109, 311, 301, 285, 89, 3, 144, 293, 156, 75, 348, 219, 333, 366, 79, 299, 24, 34, 376, 221, 263, 372, 364, 159, 190, 204, 355, 340, 371, 62, 160, 33, 253, 4, 22, 9, 273, 30, 384, 8, 295, 134, 267, 18, 154, 342, 317, 321, 308, 242, 223, 112, 252, 238, 245, 289, 76, 199, 369, 195, 151, 248, 319, 343, 165, 121, 128, 279, 113, 297, 157, 87, 322, 92, 102, 216, 74, 150, 234, 325, 268, 241, 330, 148, 281, 170, 362, 32, 218, 114, 233, 214, 393, 188, 104, 349, 153, 251, 147, 132, 95, 2, 99, 231, 312, 310, 116, 46, 117, 300, 44, 39, 359, 212, 395, 27, 50, 261, 101, 174, 68, 391, 106, 388, 130, 303, 296, 213, 345, 259, 177, 149, 172] test_idx = [335, 269, 37, 66, 133, 257, 314, 396, 21, 275, 336, 145, 370, 36, 41, 390, 220, 392, 119, 337, 17, 198, 83, 70, 137, 162, 329, 315, 205, 265, 331, 197, 332, 65, 191, 82, 361, 184, 254, 286, 163, 94, 111, 287, 260, 93, 138, 358, 71, 278, 126, 291, 208, 78, 139, 373, 43, 6, 135, 142, 192, 81, 272, 352, 250, 227, 140, 108, 324, 168, 11, 202, 386, 266, 91, 292, 207, 246, 53, 38, 56, 294, 161, 127, 341, 357, 54, 122, 173, 29, 129, 283, 164, 200, 356, 64, 31, 264, 19, 277, 206, 12, 378, 399, 240, 26, 326, 5, 175, 169, 123, 158, 367, 389, 23, 274, 136, 270, 48, 320] train_new_idx = [27, 184, 45, 155, 253, 224, 60, 38, 210, 51, 101, 114, 228, 197, 66, 88, 132, 134, 245, 70, 26, 9, 256, 6, 83, 96, 15, 99, 78, 8, 29, 130, 79, 162, 274, 202, 154, 222, 231, 46, 193, 242, 180, 116, 263, 115, 212, 135, 213, 239, 269, 144, 247, 61, 216, 181, 138, 271, 150, 173, 221, 90, 227, 4, 262, 182, 112, 105, 74, 226, 204, 118, 62, 166, 240, 149, 31, 24, 254, 163, 236, 175, 148, 207, 174, 246, 95, 80, 142, 13, 36, 123, 252, 54, 117, 196, 108, 53, 100, 244, 50, 237, 48, 260, 35, 220, 267, 129, 32, 257, 232, 157, 97, 225, 223, 49, 84, 86, 199, 23, 5, 187, 2, 219, 268, 111, 110, 87, 183, 198, 72, 230, 47, 258, 122, 119, 161, 102, 272, 205, 11, 214, 133, 215, 77, 151, 264, 124, 200, 64, 278, 158, 137, 276, 25, 113, 176, 41, 172, 92, 22, 145, 12, 277, 188, 0, 44, 42, 164, 208, 34, 141, 251, 68, 194, 1, 211, 85, 143, 243, 209, 203, 63, 81, 106, 93, 171, 67, 233, 186, 10, 71, 160, 91, 241, 190, 21, 131, 279, 28, 7, 56, 43, 18, 192, 82, 250, 167, 19, 169, 147, 261, 191, 136, 103, 65, 58, 178, 98, 206, 37, 140, 270, 75, 275, 179, 195, 20, 234, 259, 89, 16, 107, 255, 126, 73, 265, 76, 17, 127, 189, 165, 156, 248, 52, 153, 3, 94, 249, 125, 69, 229] valid_idx = [218, 30, 235, 33, 217, 120, 177, 159, 139, 185, 266, 55, 40, 128, 39, 170, 238, 201, 14, 146, 59, 109, 273, 121, 168, 57, 104, 152] . # train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1, transform=train_transform) train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler, num_workers=1) # only running the data augmentation on the training data, double check if works valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=valid_sampler, num_workers=1) test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, num_workers=1) # variable representing classes of the images classes = [0,1] # labeling either 0 (negative) or 1 (positive) total_length = len(test_loader)*batch_size + len(valid_loader)*batch_size + len(train_loader)*batch_size print(&quot;total_length = &quot;, total_length) . total_length = 416 . for batch in valid_loader: print(&quot;batch[0].size() = &quot;, batch[0].size()) . batch[0].size() = torch.Size([16, 3, 384, 384]) batch[0].size() = torch.Size([12, 3, 384, 384]) . import matplotlib.pyplot as plt %matplotlib inline # helper function to un-normalize and display an image def imshow(img): img = img / 2 + 0.5 # unnormalize plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # obtain one batch of training images dataiter = iter(train_loader) images, labels = next(dataiter) images = images.numpy() # convert images to numpy for display # plot the images in the batch, along with the corresponding labels . &lt;Figure size 1000x400 with 0 Axes&gt; . num_of_image_to_display = 16 fig = plt.figure(figsize=(num_of_image_to_display, 4)) for idx in np.arange(num_of_image_to_display): ax = fig.add_subplot(2, int(num_of_image_to_display/2), idx+1, xticks=[], yticks=[]) imshow(images[idx]) ax.set_title(classes[labels[idx]]) display(plt.show()) plt.close() . None . import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms from torch.utils.data.sampler import SubsetRandomSampler import torch.nn as nn import torch.optim as optim import torch.utils.data as data import torchvision.models as models from torch.autograd import Variable . import torch.nn as nn import torch.nn.functional as F train_on_gpu = torch.cuda.is_available() print(&quot;train_on_gpu = &quot;, train_on_gpu) class Net(nn.Module): def __init__(self): super(Net, self).__init__() # define the two convolutional operations # conv1 Input Channel =3 for the R, G, B channels directly from the Image self.conv1 = nn.Conv2d(3, convOutputChannel1, convKernelSize1) self.conv2 = nn.Conv2d(convOutputChannel1, convOutputChannel2, convKernelSize2) # define the max pool operation self.pool = nn.MaxPool2d(maxpoolKernelSize, maxpoolKernelSize) self.dropout = nn.Dropout(dropOutValue) # define the Linear (fully connected) operations self.fc1 = nn.Linear(convOutputChannel2* height2 * width2, linearOut1) self.fc2 = nn.Linear(linearOut1, linearOut2) self.fc3 = nn.Linear(linearOut2, 2) self.softmax = nn.LogSoftmax(dim=1) def forward(self, x): # add sequence of convolutional, relu and max pooling layers x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = self.dropout(x) # the linearization needs to start with = # of output channels * height2 * width2 x = x.view(-1, convOutputChannel2 * height2 * width2) x = F.relu(self.fc1(x)) x = self.dropout(F.relu(self.fc2(x))) x = self.softmax(self.fc3(x)) return x . train_on_gpu = True . Training the Model . Finally comes the training part. Here you need to decide two crucial things: Loss function and optimizer. There are various choices like SGD, Adam, etc.. for the optimizer. used Cross-Entropy loss, which is a popular choice in the case of classification problems. should also set a learning rate, which decides how fast your model learns. . model = Net() print(&quot;++++++++++++ print the model ++++++++&quot;) print(model) print(&quot;+++++++++++++++++++++++++++++++++++++&quot;) # move tensors to GPU if CUDA is available if train_on_gpu: model.cuda() . ++++++++++++ print the model ++++++++ Net( (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1)) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (dropout): Dropout(p=0.2, inplace=False) (fc1): Linear(in_features=276768, out_features=256, bias=True) (fc2): Linear(in_features=256, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=2, bias=True) (softmax): LogSoftmax(dim=1) ) +++++++++++++++++++++++++++++++++++++ . import torch.optim as optim # specify loss function criterion = torch.nn.CrossEntropyLoss() learning_rate = 0.003 # test with Stochastic gradient descent optimizer - optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum= 0.9) . n_epochs = 5 # you may increase this number to train a final model valid_loss_min = np.Inf # track change in validation loss . def make_loss_graph(data_list_val, data_list_train): plt.close() plt.title(&quot;Training and Validation Loss per Epoch&quot;) val_label = &quot;Validation Loss &quot; train_label = &quot;Training Loss &quot; assert len(data_list_val) == len(data_list_train) length = len(data_list_train) - 1 plt.plot(data_list_val, label=val_label) plt.plot(data_list_train, label=train_label) #plt.xlim((epoch_skip, length)) plt.xlabel(&#39;Epochs&#39;) plt.legend() plt.grid() plt.axis([0, 10, 0, 10]) display(plt.show()) plt.close() . list_val_loss_all_epoch =[] list_train_loss_all_epoch =[] for epoch in range(1, n_epochs+1): print(&#39;####### EPOCH &#39;, epoch) # keep track of training and validation loss train_loss = 0.0 valid_loss = 0.0 train_items = 0.0 valid_items = 0.0 ################### # train the model # ################### model.train() print(&#39; len(train_loader.dataset) = &#39;, len(train_loader.dataset)) for data, target in train_loader: # move tensors to GPU if CUDA is available if train_on_gpu: data, target = data.cuda(), target.cuda() # clear the gradients of all optimized variables optimizer.zero_grad() # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the batch loss loss = criterion(output, target) # backward pass: compute gradient of the loss with respect to model parameters loss.backward() # perform a single optimization step (parameter update) optimizer.step() # update training loss print(&#39; train loss item = &#39;, loss.item()*data.size(0)) train_loss += loss.item()*data.size(0) train_items +=1 ###################### # validate or evaluate the model # # To evaluate the model, it should be changed from model.train() to model.eval() ###################### model.eval() print(&#39; len(valid_loader.dataset) = &#39;, len(valid_loader.dataset)) for data, target in valid_loader: # move tensors to GPU if CUDA is available if train_on_gpu: data, target = data.cuda(), target.cuda() # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the batch loss loss = criterion(output, target) # update average validation loss print(&#39; valid loss item = &#39;, loss.item()*data.size(0)) valid_loss += loss.item()*data.size(0) valid_items +=1 # calculate average losses print(&#39; n train_loss = &#39;, train_loss) print(&#39; valid_loss = &#39;, valid_loss) print(&#39; n train_items = &#39;, train_items) print(&#39; valid_items = &#39;, valid_items) #train_loss = train_loss/len(train_loader.dataset) # incorrect: this was averaging over 500 (the total amount of images available) #valid_loss = valid_loss/len(valid_loader.dataset) train_loss = train_loss/train_items # this is averaging correctly valid_loss = valid_loss/valid_items print(&#39; n average train_loss = &#39;, train_loss) print(&#39; average valid_loss = &#39;, valid_loss) list_val_loss_all_epoch.append(valid_loss) list_train_loss_all_epoch.append(train_loss) # print training/validation statistics print(&#39;Epoch: {} t Average Training Loss: {:.6f} t Average Validation Loss: {:.6f}&#39;.format( epoch, train_loss, valid_loss)) # save model if validation loss has decreased if valid_loss &lt;= valid_loss_min: print(&#39;Validation loss decreased ({:.6f} --&gt; {:.6f}). Saving model ...&#39;.format( valid_loss_min, valid_loss)) torch.save(model.state_dict(), &#39;model_cifar.pt&#39;) valid_loss_min = valid_loss make_loss_graph(list_val_loss_all_epoch, list_train_loss_all_epoch) . ####### EPOCH 1 len(train_loader.dataset) = 400 train loss item = 0.024645444005727768 train loss item = 0.32031652331352234 train loss item = 0.05283474177122116 train loss item = 0.9114115238189697 train loss item = 0.04939626529812813 train loss item = 0.03176403045654297 train loss item = 0.02037661150097847 train loss item = 0.11796353757381439 train loss item = 0.4087546467781067 train loss item = 0.12971322238445282 train loss item = 0.2309323251247406 train loss item = 0.17112958431243896 train loss item = 0.00805441290140152 train loss item = 0.7278347015380859 train loss item = 0.03808647766709328 train loss item = 0.001081527181668207 len(valid_loader.dataset) = 400 valid loss item = 0.3364920914173126 valid loss item = 0.7835705280303955 train_loss = 3.244295575626893 valid_loss = 1.1200626194477081 train_items = 16.0 valid_items = 2.0 average train_loss = 0.2027684734766808 average valid_loss = 0.5600313097238541 Epoch: 1 Average Training Loss: 0.202768 Average Validation Loss: 0.560031 Validation loss decreased (1.058500 --&gt; 0.560031). Saving model ... ####### EPOCH 2 len(train_loader.dataset) = 400 train loss item = 0.017503857612609863 train loss item = 0.012020493857562542 train loss item = 1.2742114067077637 train loss item = 0.02505093812942505 train loss item = 0.03209812566637993 train loss item = 0.16146953403949738 train loss item = 0.1393091380596161 train loss item = 1.8395599126815796 train loss item = 6.752047061920166 train loss item = 0.016535572707653046 train loss item = 0.012823164463043213 train loss item = 0.04886973649263382 train loss item = 0.11911410838365555 train loss item = 0.8541668057441711 train loss item = 8.914411544799805 train loss item = 0.007184454007074237 len(valid_loader.dataset) = 400 valid loss item = 5.69299840927124 valid loss item = 0.004451676271855831 train_loss = 20.226375855272636 valid_loss = 5.697450085543096 train_items = 16.0 valid_items = 2.0 average train_loss = 1.2641484909545397 average valid_loss = 2.848725042771548 Epoch: 2 Average Training Loss: 1.264148 Average Validation Loss: 2.848725 ####### EPOCH 3 len(train_loader.dataset) = 400 train loss item = 0.28011396527290344 train loss item = 0.23054836690425873 train loss item = 0.05393335595726967 train loss item = 1.2450270652770996 train loss item = 0.08126480132341385 train loss item = 0.5129839181900024 train loss item = 0.4966256022453308 train loss item = 0.08642233908176422 train loss item = 1.2715575695037842 train loss item = 0.1827414333820343 train loss item = 0.20200985670089722 train loss item = 0.2556696832180023 train loss item = 0.10684928297996521 train loss item = 0.11533006280660629 train loss item = 0.12136797606945038 train loss item = 0.1404786966741085 len(valid_loader.dataset) = 400 valid loss item = 0.19773808121681213 valid loss item = 0.9970040023326874 train_loss = 5.382923975586891 valid_loss = 1.1947420835494995 train_items = 16.0 valid_items = 2.0 average train_loss = 0.3364327484741807 average valid_loss = 0.5973710417747498 Epoch: 3 Average Training Loss: 0.336433 Average Validation Loss: 0.597371 ####### EPOCH 4 len(train_loader.dataset) = 400 train loss item = 0.20712696015834808 train loss item = 0.07195495814085007 train loss item = 0.14663034677505493 train loss item = 0.1379862129688263 train loss item = 0.11275184154510498 train loss item = 0.16099438071250916 train loss item = 0.08503907173871994 train loss item = 0.0856439545750618 train loss item = 0.29068124294281006 train loss item = 0.017780523747205734 train loss item = 0.05050569772720337 train loss item = 0.011246606707572937 train loss item = 0.0094155790284276 train loss item = 0.059190504252910614 train loss item = 0.027335597202181816 train loss item = 0.02183528244495392 len(valid_loader.dataset) = 400 valid loss item = 0.02724332921206951 valid loss item = 0.4153520464897156 train_loss = 1.4961187606677413 valid_loss = 0.4425953757017851 train_items = 16.0 valid_items = 2.0 average train_loss = 0.09350742254173383 average valid_loss = 0.22129768785089254 Epoch: 4 Average Training Loss: 0.093507 Average Validation Loss: 0.221298 Validation loss decreased (0.560031 --&gt; 0.221298). Saving model ... ####### EPOCH 5 len(train_loader.dataset) = 400 train loss item = 0.29646751284599304 train loss item = 0.06760089844465256 train loss item = 0.07073017954826355 train loss item = 0.053998708724975586 train loss item = 0.005736634135246277 train loss item = 0.0050850375555455685 train loss item = 0.0208502858877182 train loss item = 0.246884286403656 train loss item = 0.015171251259744167 train loss item = 0.02811436913907528 train loss item = 0.003049638122320175 train loss item = 0.001593883614987135 train loss item = 0.001167911570519209 train loss item = 0.1199827566742897 train loss item = 0.02852724678814411 train loss item = 1.006018728017807 len(valid_loader.dataset) = 400 valid loss item = 0.4509490430355072 valid loss item = 0.032683937810361385 train_loss = 1.9709793287329376 valid_loss = 0.4836329808458686 train_items = 16.0 valid_items = 2.0 average train_loss = 0.1231862080458086 average valid_loss = 0.2418164904229343 Epoch: 5 Average Training Loss: 0.123186 Average Validation Loss: 0.241816 . None . # Analysis of the Model # plotting loss and accuracy over epochs to see how it changed over training ###################################################### print(&quot; n **************** START TEST ******************* &quot;) # plot the accuracy # confusion matrix # dont train for 700 epochs - do for less ~20 # track test loss test_loss = 0.0 class_correct = list(0. for i in range(2)) class_total = list(0. for i in range(2)) model.eval() i=1 # iterate over test data len(test_loader) for data, target in test_loader: i=i+1 if len(target)!=batch_size: continue # move tensors to GPU if CUDA is available if train_on_gpu: data, target = data.cuda(), target.cuda() # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the batch loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct_tensor = pred.eq(target.data.view_as(pred)) correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy()) # calculate test accuracy for each object class # print(target) for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # average test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f} n&#39;.format(test_loss)) for i in range(2): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39; nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) . **************** START TEST ******************* Test Loss: 0.030040 Test Accuracy of 0: 95% (57/60) Test Accuracy of 1: 94% (49/52) Test Accuracy (Overall): 94% (106/112) .",
            "url": "https://rohitd3.github.io/manyFacesML/2023/05/20/CNN_faces.html",
            "relUrl": "/2023/05/20/CNN_faces.html",
            "date": " • May 20, 2023"
        }
        
    
  

  
  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rohitd3.github.io/manyFacesML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}